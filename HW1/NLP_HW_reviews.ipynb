{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Парсинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы нам понадобятся отзывы на фильм \"Начало\" с Кинопоиска. Сначала пропишем несколько функций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_processing(url): #обработка запроса\n",
    "    session = requests.session()\n",
    "    time.sleep(1)\n",
    "    req = session.get(url)\n",
    "    time.sleep(2)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(first_link): #получение ссылок страниц\n",
    "    pages_list = [first_link]\n",
    "    page = url_processing(first_link)\n",
    "    for link in page.find_all('a'):\n",
    "        time.sleep(1)\n",
    "        actualink = link.get('href')\n",
    "        if actualink.endswith(\"page/2/\") or actualink.endswith(\"page/3/\"):\n",
    "            flink = \"https://www.kinopoisk.ru\"+ actualink\n",
    "            pages_list.append(flink)\n",
    "    return pages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_30revs(pages): #получаем 30 отзывов\n",
    "    gorb30revs = []\n",
    "    for page_link in pages:\n",
    "        page_revs = url_processing(page_link)\n",
    "        for rev in page_revs.find_all('span', {'class': '_reachbanner_'}):\n",
    "            time.sleep(1)\n",
    "            gorb30revs.append(rev.text)\n",
    "    return gorb30revs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, сначала обрабатываем ссылку с отзывами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "revs = url_processing(\"https://www.kinopoisk.ru/film/447301/reviews/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обработанной ссылке можно найти отдельно ссылки с хорошими и плохими отзывами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "good_revs_link = \"\" #сюда запишется ссылка с хорошими отзывами\n",
    "bad_revs_link = \"\" #а сюда — с плохими\n",
    "for rlink in revs.find_all('a'):\n",
    "    time.sleep(1)\n",
    "    gorb_link = rlink.get(\"href\")\n",
    "    if \"good\" in gorb_link:\n",
    "        good_revs_link = \"https://www.kinopoisk.ru\" + gorb_link\n",
    "    elif \"bad\" in gorb_link:\n",
    "        bad_revs_link = \"https://www.kinopoisk.ru\" + gorb_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в одной, и в другой ссылках на каждой странице по 10 отзывов. Поэтому, в фунцкии get_pages нам достаточно получить по 3 страницы. Начнем с положительных отзывов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gpage_urls = list(set(get_pages(good_revs_link))) #использовали set, чтобы избавиться от дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g30revs = get_30revs(gpage_urls) #здесь все 30 хороших отзывов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдем к отрицательным отзывам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bpage_urls = list(set(get_pages(bad_revs_link)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "b30revs = get_30revs(bpage_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда даже с time.sleep() обработка данных прерывается из-за блокировок, поэтому будет неплохо записать однажды полученные отзывы в файлы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grevs.txt\", \"w\", encoding=\"utf-8\") as f: #положительные отзывы\n",
    "    for text in g30revs:\n",
    "        f.write(text)\n",
    "        f.write(\"=====\") #разделитель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brevs.txt\", \"w\", encoding=\"utf-8\") as f: #отрицательные отзывы\n",
    "    for text in b30revs:\n",
    "        f.write(text)\n",
    "        f.write(\"=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Работа с текстами из файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tknzd_lems(file): #лемматизация, токенизация\n",
    "    lems = []\n",
    "    freqlist = Counter()\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_revs = f.read().split(\"=====\")\n",
    "        for text in all_revs:\n",
    "            for word in nltk.word_tokenize(text.lower()):\n",
    "                lem = morph.parse(word)[0].normal_form\n",
    "                lems.append(lem)\n",
    "    return lems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_borg_lems(lems1, lems2): #составление 2 множеств со словами, которые втсречаются только в хороших/плохих отзывах\n",
    "    onelist_lems = []\n",
    "    for lem in lems1.items():\n",
    "        if lem[0] not in lems2:  \n",
    "            onelist_lems.append(lem)\n",
    "    return dict(set(onelist_lems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr_more_than2(fr_lems): #избавляемся от лемм, встретившихся меньше 3 раз\n",
    "    lemset = []\n",
    "    for lem_fr in fr_lems.items():\n",
    "        if lem_fr[1] > 2:\n",
    "            lemset.append(lem_fr)\n",
    "    return dict(lemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "glems = tknzd_lems(\"grevs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "blems = tknzd_lems(\"brevs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_glems = collections.Counter(glems)\n",
    "fr_blems = collections.Counter(blems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "glems_mt2 = fr_more_than2(fr_glems)\n",
    "blems_mt2 = fr_more_than2(fr_blems)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyglems = set_borg_lems(glems_mt2, blems_mt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyblems = set_borg_lems(blems_mt2, glems_mt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_freq_lists = {}\n",
    "bg_freq_lists[\"good\"] = onlyglems\n",
    "bg_freq_lists[\"bad\"] = onlyblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Определялка и ее точность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_or_good_detect(freq_lists, text):\n",
    "    counts = Counter()\n",
    "    for b_or_g, freq_list in freq_lists.items():\n",
    "        freq_list = Counter(freq_list)\n",
    "        for word in nltk.word_tokenize(text):\n",
    "            counts[b_or_g] += int(freq_list[word] > 0)\n",
    "    return counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закинем все тексты отзывов в словарь с ключами good и bad соответственно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = {}\n",
    "with open(\"grevs.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_grevs = f.read().split(\"=====\")\n",
    "    all_reviews[\"good\"] = all_grevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brevs.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_brevs = f.read().split(\"=====\")\n",
    "    all_reviews[\"bad\"] = all_brevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []  \n",
    "original_data = []  \n",
    "for bg_revs in all_reviews.items():\n",
    "    for rev in bg_revs[1]:\n",
    "        if len(rev) != 0:\n",
    "            result = bad_or_good_detect(bg_freq_lists, rev)[0][0]\n",
    "            results.append(result)\n",
    "            original_data.append(bg_revs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accr = accuracy_score(results, original_data)\n",
    "accr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На мой взгляд, довольно странно видеть такой точный результат, для лучшего понимания работы accuracy было бы хорошо обработать большее количество текстов. Также стоит поразмышлять, возможно ли было б ускорить работу самого парсинга, например, избежать if/elif."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
